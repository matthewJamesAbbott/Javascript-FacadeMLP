<!--
  This HTML+JavaScript file provides an in-browser MLP (Multi-Layer Perceptron) trainer. It includes:
   - UI to pick network shape/layer/activation
   - Upload, paste, or generate training datasets
   - Save/Load trained models
   - Train (with progress reporting)
   - Predict on ad-hoc user inputs
   - A Layer Facade for flexible layer/weight/activation access
   - Modular and highly-commented functions for learning and teaching purposes
-->

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Multi-Layer Perceptron Trainer with Layer Facade</title>
    <style>
        /* ...STYLE OMITTED FOR BREVITY... */
    </style>
</head>
<body>
    <h1>Multi-Layer Perceptron Trainer with Layer Facade</h1>

    <!-- ===== User controls for network architecture and training ===== -->
    <div class="section">
        <h2>Network Configuration</h2>
        <label>Input Size: <input type="number" id="inputSize" value="10" min="1"></label>
        <label>Hidden Layer Sizes (comma separated): <input type="text" id="hiddenSizes" value="8,8,8"></label>
        <label>Output Size: <input type="number" id="outputSize" value="3" min="1"></label>
        <label>Hidden Layer Activation:
            <select id="hiddenActivation">
                <option value="sigmoid" selected>Sigmoid</option>
                <option value="tanh">Tanh</option>
                <option value="relu">ReLU</option>
            </select>
        </label>
        <label>Output Layer Activation:
            <select id="outputActivation">
                <option value="softmax" selected>Softmax</option>
                <option value="sigmoid">Sigmoid</option>
                <option value="tanh">Tanh</option>
                <option value="relu">ReLU</option>
            </select>
        </label>
        <label>Learning Rate: <input type="number" id="learningRate" value="0.1" step="0.01" min="0.001" max="1"></label>
        <button onclick="createNetwork()">Create Network</button>
        <span id="networkStatus" style="margin-left:1rem;"></span>
    </div>

    <!-- ===== Training meta-parameters ===== -->
    <div class="section">
        <h2>Training Configuration</h2>
        <div class="epoch-controls">
            <label>Training Epochs: <input type="number" id="trainingEpochs" value="50" min="1"></label>
            <label>Validation Epochs: <input type="number" id="validationEpochs" value="30" min="1"></label>
            <label>Batch Size:
                <select id="batchSize">
                    <option value="1">1 (Online Learning)</option>
                    <option value="32" selected>32</option>
                    <option value="64">64</option>
                    <option value="128">128</option>
                    <option value="all">Full Batch</option>
                </select>
            </label>
        </div>
        <div class="epoch-info">
            ℹ️ Training Epochs: full passes through training data <br>
            ℹ️ Validation Epochs: used in k-fold cross-validation <br>
            ℹ️ Batch Size: samples processed before weights update
        </div>
    </div>

    <!-- ====== Dataset input and generators ======= -->
    <div class="section">
        <h2>Load Training Data</h2>
        <input type="file" id="csvFile" accept=".csv">
        <button onclick="loadCSV()">Load CSV</button>
        <span id="dataStatus"></span>
        <br>
        <label>Or paste CSV data:</label>
        <textarea id="csvPaste" rows="5" cols="60"></textarea>
        <button onclick="loadCSVText()">Load Pasted CSV</button>
    </div>
    <div class="section">
        <h2>Generate Test Data</h2>
        <label>Number of Samples per Class: <input type="number" id="samplesPerClass" value="2500" min="100"></label>
        <button onclick="generateTestData()">Generate Test Data</button>
        <span id="generateStatus"></span>
    </div>

    <!-- ===== Model evaluation via K-fold & metrics ===== -->
    <div class="section">
        <h2>Model Evaluation</h2>
        <label>Number of Folds: <input type="number" id="numFolds" value="10" min="2" max="20"></label>
        <button onclick="runKFoldValidation()">Run K-Fold Cross Validation</button>
        <button onclick="runAllMetrics()">Run All Evaluation Metrics</button>
        <div id="evaluationResults"></div>
    </div>

    <!-- ===== Save/load neural net to/from file ===== -->
    <div class="section">
        <h2>Save/Load Model</h2>
        <button onclick="saveModel()">Save Model</button>
        <button onclick="loadModel()">Load Model</button>
        <input type="file" id="modelFile" accept=".json">
        <span id="modelStatus"></span>
    </div>

    <!-- ===== Train and show progress ===== -->
    <div class="section">
        <h2>Train Network</h2>
        <button onclick="trainNetwork()">Train Network</button>
        <button onclick="trainNetworkWithProgress()">Train with Progress</button>
        <div id="trainStatus"></div>
        <div id="trainingProgress"></div>
    </div>

    <!-- ===== Manual user prediction ===== -->
    <div class="section">
        <h2>Predict</h2>
        <label>Input (comma separated): <input type="text" id="predictInput" value=""></label>
        <button onclick="predictInput()">Predict</button>
        <div id="predictOutput"></div>
    </div>

<!-- ====== MAIN MLP/JS CORE: Classes + Facade ====== -->
<script>
/*
    LayerFacade:
    Streamlines interaction with layers for modularity and decoupling.
    Provides unified set/get for neuron outputs, errors, biases, weights.
*/
function LayerFacade(inputLayer, hiddenLayers, outputLayer) {
    this.layers = [inputLayer].concat(hiddenLayers).concat([outputLayer]);
}
LayerFacade.prototype.getNeuronOutput = function (layerIdx, neuronIdx) {
    return this.layers[layerIdx].neurons[neuronIdx].output;
};
LayerFacade.prototype.setNeuronOutput = function (layerIdx, neuronIdx, val) {
    this.layers[layerIdx].neurons[neuronIdx].output = val;
};
LayerFacade.prototype.getNeuronError = function (layerIdx, neuronIdx) {
    return this.layers[layerIdx].neurons[neuronIdx].error;
};
LayerFacade.prototype.setNeuronError = function (layerIdx, neuronIdx, val) {
    this.layers[layerIdx].neurons[neuronIdx].error = val;
};
LayerFacade.prototype.getNeuronBias = function (layerIdx, neuronIdx) {
    return this.layers[layerIdx].neurons[neuronIdx].bias;
};
LayerFacade.prototype.setNeuronBias = function (layerIdx, neuronIdx, val) {
    this.layers[layerIdx].neurons[neuronIdx].bias = val;
};
LayerFacade.prototype.getNeuronWeight = function (layerIdx, neuronIdx, weightIdx) {
    return this.layers[layerIdx].neurons[neuronIdx].weights[weightIdx];
};
LayerFacade.prototype.setNeuronWeight = function (layerIdx, neuronIdx, weightIdx, val) {
    this.layers[layerIdx].neurons[neuronIdx].weights[weightIdx] = val;
};
LayerFacade.prototype.getLayerSize = function (layerIdx) {
    return this.layers[layerIdx].neurons.length;
};
LayerFacade.prototype.getNumLayers = function () {
    return this.layers.length;
};
LayerFacade.prototype.getNumWeights = function (layerIdx, neuronIdx) {
    return this.layers[layerIdx].neurons[neuronIdx].weights.length;
};

// ==== Common Activation Functions & Their Derivatives ====
var ActivationFunctions = {
    sigmoid: function(x) { return 1 / (1 + Math.exp(-x)); },
    dSigmoid: function(x) { return x * (1 - x); },
    tanh: function(x) { return Math.tanh(x); },
    dTanh: function(x) { return 1 - (x * x); },
    relu: function(x) { return Math.max(0, x); },
    dRelu: function(x) { return x > 0 ? 1 : 0; },
    softmax: function(xs) {
        var sum = 0, result = [], max = Math.max.apply(null, xs);
        for (var i = 0; i < xs.length; i++) sum += Math.exp(xs[i] - max);
        for (var i = 0; i < xs.length; i++) result[i] = Math.exp(xs[i] - max) / sum;
        return result;
    },
    dSoftmax: function(labels, xs) {
        // Shape: dLoss/dz for cross-entropy+softmax
        var res = [];
        for (var i = 0; i < xs.length; i++) res[i] = labels[i] - xs[i];
        return res;
    },
    // Apply activation generically by name
    apply: function(type, x) {
        if (type === 'sigmoid') return ActivationFunctions.sigmoid(x);
        if (type === 'tanh') return ActivationFunctions.tanh(x);
        if (type === 'relu') return ActivationFunctions.relu(x);
        if (type === 'softmax') return ActivationFunctions.softmax(x);
        return ActivationFunctions.sigmoid(x);
    },
    applyDerivative: function(type, x, target) {
        if (type === 'sigmoid') return ActivationFunctions.dSigmoid(x);
        if (type === 'tanh') return ActivationFunctions.dTanh(x);
        if (type === 'relu') return ActivationFunctions.dRelu(x);
        if (type === 'softmax') return target ? ActivationFunctions.dSoftmax(target, x) : x;
        return ActivationFunctions.dSigmoid(x);
    }
};

// ====== Minimal DataPoint, Neuron, Layer classes ======
function DataPoint(input, target) {
    this.input = input;
    this.target = target;
}
// Each neuron stores its weights, bias, output, backprop error
function Neuron() {
    this.weights = [];
    this.bias = 0;
    this.output = 0;
    this.error = 0;
}
// A neural net layer holds an array of Neurons and an activation type
function Layer() {
    this.neurons = [];
    this.activationType = 'sigmoid'; // Default type
}

/*
    Main MLP (multi-layer perceptron) network structure.
    Accepts:
     - input size
     - array of hidden layer sizes
     - output size
     - activation functions for hidden and output layers
     - all layer weight/biases are initialized randomly upon build
*/
function MLP(inputSize, hiddenSizes, outputSize, hiddenActivation, outputActivation) {
    this.learningRate = 0.1;
    this.inputLayer = new Layer();
    this.hiddenLayers = [];
    this.outputLayer = new Layer();
    this.hiddenActivation = hiddenActivation || 'sigmoid';
    this.outputActivation = outputActivation || 'sigmoid';
    this._initLayers(inputSize, hiddenSizes, outputSize);
    this.facade = new LayerFacade(this.inputLayer, this.hiddenLayers, this.outputLayer);
}

// === LAYER/BRAIN BUILDER ===
MLP.prototype._initLayers = function(inputSize, hiddenSizes, outputSize) {
    // Build input layer (dummy as it's always a "pass-through")
    this.inputLayer.neurons = [];
    for (var i = 0; i < inputSize; i++) {
        var n = new Neuron(); n.output = 0;
        this.inputLayer.neurons.push(n);
    }
    // Hidden layers (with own act type)
    var prevSize = inputSize;
    this.hiddenLayers = [];
    for (var h = 0; h < hiddenSizes.length; h++) {
        var layer = new Layer();
        layer.activationType = this.hiddenActivation;
        for (var i = 0; i < hiddenSizes[h]; i++) {
            var neuron = new Neuron();
            neuron.weights = randomWeights(prevSize);
            neuron.bias = (Math.random() * 2 - 1) * 0.1;
            layer.neurons.push(neuron);
        }
        this.hiddenLayers.push(layer);
        prevSize = hiddenSizes[h];
    }
    // Output layer (activation set by the output dropdown)
    this.outputLayer = new Layer();
    this.outputLayer.activationType = this.outputActivation;
    for (var i = 0; i < outputSize; i++) {
        var neuron = new Neuron();
        neuron.weights = randomWeights(prevSize);
        neuron.bias = (Math.random() * 2 - 1) * 0.1;
        this.outputLayer.neurons.push(neuron);
    }
};

// Utility: Random weights between -limit and +limit
function randomWeights(size) {
    var limit = Math.sqrt(6 / size), w = [];
    for (var i = 0; i < size; i++) {
        w.push((Math.random() * 2 - 1) * limit);
    }
    return w;
}

/* 
    Forward pass using the facade:
    1. Set input layer outputs.
    2. For each hidden layer, multiply previous layer’s outputs by weights, add bias, apply activation.
    3. For output layer, either run through softmax or standard activation.
    4. Save all neuron outputs for backwards weight update.
*/
MLP.prototype.feedForward = function(input) {
    var i, j, sum;
    // Set input neuron outputs directly
    for (i = 0; i < this.inputLayer.neurons.length; i++) {
        this.inputLayer.neurons[i].output = input[i];
    }
    var prevOutputs = input;
    // Hidden layers
    var facade = this.facade;
    for (var layerIdx = 1; layerIdx <= this.hiddenLayers.length; layerIdx++) {
        var curOutputs = [];
        var layer = facade.layers[layerIdx];
        for (i = 0; i < layer.neurons.length; i++) {
            sum = layer.neurons[i].bias;
            for (j = 0; j < prevOutputs.length; j++) {
                sum += prevOutputs[j] * layer.neurons[i].weights[j];
            }
            layer.neurons[i].output = ActivationFunctions.apply(layer.activationType, sum);
            curOutputs.push(layer.neurons[i].output);
        }
        prevOutputs = curOutputs;
    }
    // Output layer
    var outputSums = [];
    for (i = 0; i < this.outputLayer.neurons.length; i++) {
        sum = this.outputLayer.neurons[i].bias;
        for (j = 0; j < prevOutputs.length; j++) {
            sum += prevOutputs[j] * this.outputLayer.neurons[i].weights[j];
        }
        outputSums.push(sum);
    }
    var output;
    // If softmax, use joint probabilities; if not, ordinary activations
    if (this.outputActivation === 'softmax') {
        output = ActivationFunctions.softmax(outputSums);
        for (i = 0; i < this.outputLayer.neurons.length; i++) {
            this.outputLayer.neurons[i].output = output[i];
        }
    } else {
        output = [];
        for (i = 0; i < this.outputLayer.neurons.length; i++) {
            this.outputLayer.neurons[i].output = ActivationFunctions.apply(this.outputActivation, outputSums[i]);
            output.push(this.outputLayer.neurons[i].output);
        }
    }
    return output;
};

MLP.prototype.predict = function(inputArr) {
    return this.feedForward(inputArr);
};

/*
    Backpropagation (training step):
    1. Calculate error in output layer.
    2. Work backward through hidden layers, computing deltas for each neuron.
    3. Save all errors for forward pass for use in weight updates.
*/
MLP.prototype.backPropagate = function(target) {
    // Output layer error
    if (this.outputActivation === 'softmax') {
        var derivatives = ActivationFunctions.dSoftmax(target, this.outputLayer.neurons.map(function(n){return n.output;}));
        for (var i = 0; i < this.outputLayer.neurons.length; i++) {
            this.outputLayer.neurons[i].error = derivatives[i];
        }
    } else {
        for (var i = 0; i < this.outputLayer.neurons.length; i++) {
            var o = this.outputLayer.neurons[i].output;
            var derivative = ActivationFunctions.applyDerivative(this.outputActivation, o);
            this.outputLayer.neurons[i].error = derivative * (target[i] - o);
        }
    }
    // Hidden layers: propagate error from above layer, use each neuron's output in derivative
    for (var layerIdx = this.hiddenLayers.length - 1; layerIdx >= 0; layerIdx--) {
        var layer = this.hiddenLayers[layerIdx];
        for (var i = 0; i < layer.neurons.length; i++) {
            var neuron = layer.neurons[i];
            var sum = 0;
            if (layerIdx === this.hiddenLayers.length - 1) {
                // Output layer is next in chain
                for (var j = 0; j < this.outputLayer.neurons.length; j++) {
                    sum += this.outputLayer.neurons[j].error * this.outputLayer.neurons[j].weights[i];
                }
            } else {
                // Next hidden layer
                for (var j = 0; j < this.hiddenLayers[layerIdx+1].neurons.length; j++) {
                    sum += this.hiddenLayers[layerIdx+1].neurons[j].error * this.hiddenLayers[layerIdx+1].neurons[j].weights[i];
                }
            }
            var derivative = ActivationFunctions.applyDerivative(layer.activationType, neuron.output);
            neuron.error = derivative * sum;
        }
    }
};

/*
    Given all neuron outputs/errors, update weights and biases.
    For each neuron, update:
      - each weight by LR * error * "input"
      - each bias by LR * error
    This is done for all output and hidden layers.
*/
MLP.prototype.updateWeights = function(input) {
    var layerOutputs = [input];
    for (var k = 0; k < this.hiddenLayers.length; k++) {
        var outputs = [];
        for (var i = 0; i < this.hiddenLayers[k].neurons.length; i++) {
            outputs.push(this.hiddenLayers[k].neurons[i].output);
        }
        layerOutputs.push(outputs);
    }
    // Output layer: Update by outputs of last hidden
    var lastHiddenOutputs = layerOutputs[layerOutputs.length-1];
    for (var i = 0; i < this.outputLayer.neurons.length; i++) {
        var neuron = this.outputLayer.neurons[i];
        for (var j = 0; j < neuron.weights.length; j++) {
            neuron.weights[j] += this.learningRate * neuron.error * lastHiddenOutputs[j];
        }
        neuron.bias += this.learningRate * neuron.error;
    }
    // Each hidden layer, from last to first
    for (var k = this.hiddenLayers.length - 1; k >= 0; k--) {
        var prevOutputs = layerOutputs[k];
        for (var i = 0; i < this.hiddenLayers[k].neurons.length; i++) {
            var neuron = this.hiddenLayers[k].neurons[i];
            for (var j = 0; j < neuron.weights.length; j++) {
                neuron.weights[j] += this.learningRate * neuron.error * prevOutputs[j];
            }
            neuron.bias += this.learningRate * neuron.error;
        }
    }
};

MLP.prototype.train = function(input, target) {
    this.feedForward(input);
    this.backPropagate(target);
    this.updateWeights(input);
};

/* Model persistence (save/load as JSON for later reuse, re-creation, or export) */
MLP.prototype.toJSON = function() {
    return JSON.stringify({
        learningRate: this.learningRate,
        inputSize: this.inputLayer.neurons.length,
        hiddenSizes: this.hiddenLayers.map(function(l){return l.neurons.length;}),
        outputSize: this.outputLayer.neurons.length,
        hiddenActivation: this.hiddenActivation,
        outputActivation: this.outputActivation,
        hiddenLayers: this.hiddenLayers.map(function(layer){
            return {
                activationType: layer.activationType,
                neurons: layer.neurons.map(function(n){
                    return { weights: n.weights, bias: n.bias };
                })
            };
        }),
        outputLayer: {
            activationType: this.outputLayer.activationType,
            neurons: this.outputLayer.neurons.map(function(n){
                return { weights: n.weights, bias: n.bias };
            })
        }
    });
};
MLP.fromJSON = function(jsonStr) {
    var obj = JSON.parse(jsonStr);
    var mlp = new MLP( obj.inputSize,
        obj.hiddenSizes,
        obj.outputSize,
        obj.hiddenActivation,
        obj.outputActivation
    );
    mlp.learningRate = obj.learningRate;
    for (var k = 0; k < mlp.hiddenLayers.length; k++) {
        if (obj.hiddenLayers[k].activationType) mlp.hiddenLayers[k].activationType = obj.hiddenLayers[k].activationType;
        for (var i = 0; i < mlp.hiddenLayers[k].neurons.length; i++) {
            mlp.hiddenLayers[k].neurons[i].weights = obj.hiddenLayers[k].neurons[i].weights;
            mlp.hiddenLayers[k].neurons[i].bias = obj.hiddenLayers[k].neurons[i].bias;
        }
    }
    if (obj.outputLayer.activationType)
        mlp.outputLayer.activationType = obj.outputLayer.activationType;
    for (var i = 0; i < mlp.outputLayer.neurons.length; i++) {
        mlp.outputLayer.neurons[i].weights = obj.outputLayer.neurons[i].weights;
        mlp.outputLayer.neurons[i].bias = obj.outputLayer.neurons[i].bias;
    }
    return mlp;
};

// ======= DATA HANDLING, UI & TRAINING =====
var mlp = null;
var data = [];
function parseCSV(text) {
    var lines = text.trim().split("\n");
    var points = [];
    for (var i = 0; i < lines.length; i++) {
        if (!lines[i].trim()) continue;
        var arr = lines[i].split(",").map(Number);
        var inputSize = parseInt(document.getElementById("inputSize").value, 10);
        var outputSize = parseInt(document.getElementById("outputSize").value, 10);
        var input = arr.slice(0, inputSize);
        var target = arr.slice(inputSize, inputSize + outputSize);
        points.push(new DataPoint(input, target));
    }
    return points;
}

function loadCSV() {
    var file = document.getElementById("csvFile").files[0];
    if (!file) return;
    var reader = new FileReader();
    reader.onload = function(e) {
        data = parseCSV(e.target.result);
        document.getElementById("dataStatus").textContent = "CSV loaded: " + data.length + " records.";
    };
    reader.readAsText(file);
}
function loadCSVText() {
    var txt = document.getElementById("csvPaste").value;
    data = parseCSV(txt);
    document.getElementById("dataStatus").textContent = "CSV loaded: " + data.length + " records.";
}

function generateTestData() {
    var samplesPerClass = parseInt(document.getElementById("samplesPerClass").value, 10);
    var inputSize = parseInt(document.getElementById("inputSize").value, 10);
    var outputSize = parseInt(document.getElementById("outputSize").value, 10);
    data = [];
    for (var classIdx = 0; classIdx < outputSize; classIdx++) {
        for (var i = 0; i < samplesPerClass; i++) {
            var input = [];
            var target = [];
            // targets one-hot
            for (var t = 0; t < outputSize; t++) target[t] = 0;
            target[classIdx] = 1;
            // Generate class-differentiated inputs (e.g., cluster-like)
            for (var j = 0; j < inputSize; j++) {
                if (classIdx === 0) input.push(Math.random() * 0.5);
                else if (classIdx === 1) input.push(0.5 + Math.random() * 0.5);
                else if (classIdx === 2) input.push(j % 2 === 0 ? Math.random() * 0.5 : 0.5 + Math.random() * 0.5);
                else input.push(Math.random());
            }
            data.push(new DataPoint(input, target));
        }
    }
    // Shuffle
    for (var k = data.length - 1; k > 0; k--) {
        var j = Math.floor(Math.random() * (k + 1));
        var tmp = data[k]; data[k] = data[j]; data[j] = tmp;
    }
    document.getElementById("generateStatus").textContent = "Generated " + data.length + " records.";
}

// Build, from UI selections, a new MLP instance
function createNetwork() {
    var inputSize = parseInt(document.getElementById("inputSize").value, 10);
    var hiddenSizes = document.getElementById("hiddenSizes").value.split(",").map(function(x){return parseInt(x.trim(), 10);});
    var outputSize = parseInt(document.getElementById("outputSize").value, 10);
    var hiddenAct = document.getElementById("hiddenActivation").value;
    var outputAct = document.getElementById("outputActivation").value;
    mlp = new MLP(inputSize, hiddenSizes, outputSize, hiddenAct, outputAct);
    mlp.learningRate = parseFloat(document.getElementById("learningRate").value);
    document.getElementById("networkStatus").textContent = "Network created!";
}

/* -- K-Fold Cross Validation: divides dataset into k groups for robust scoring, reporting average -- */
function runKFoldValidation() {
    var numFolds = parseInt(document.getElementById("numFolds").value, 10);
    var validationEpochs = parseInt(document.getElementById("validationEpochs").value, 10);
    var numSamples = data.length;
    var foldSize = Math.floor(numSamples / numFolds);
    var sumAccuracy = 0;
    for (var i = 0; i < numFolds; i++) {
        var foldMLP = MLP.fromJSON(mlp.toJSON()); // fresh for each fold
        var testSet = data.slice(i * foldSize, (i + 1) * foldSize);
        var trainSet = data.slice(0, i * foldSize).concat(data.slice((i + 1) * foldSize));
        for (var ep = 0; ep < validationEpochs; ep++) {
            for (var k = 0; k < trainSet.length; k++) {
                foldMLP.train(trainSet[k].input, trainSet[k].target);
            }
        }
        var correctPred = 0;
        for (var j = 0; j < testSet.length; j++) {
            var pred = foldMLP.predict(testSet[j].input);
            var predictedClass = getMaxIndex(pred);
            var actualClass = getMaxIndex(testSet[j].target);
            if (predictedClass === actualClass) correctPred++;
        }
        sumAccuracy += correctPred;
    }
    var acc = sumAccuracy / numSamples;
    document.getElementById("evaluationResults").innerHTML = "<div class='metric'>K-Fold Accuracy: " + acc.toFixed(4) + "</div>";
}

function runAllMetrics() {
    var outputSize = mlp.outputLayer.neurons.length;
    var resultsHtml = "<table><tr><th>Class</th><th>Precision</th><th>Recall</th><th>F1 Score</th></tr>";
    for (var c = 0; c < outputSize; c++) {
        var prec = precisionScore(data, mlp, c);
        var rec = recallScore(data, mlp, c);
        var f1 = f1Score(prec, rec);
        resultsHtml += "<tr><td>" + c + "</td><td>" + prec.toFixed(3) + "</td><td>" + rec.toFixed(3) + "</td><td>" + f1.toFixed(3) + "</td></tr>";
    }
    resultsHtml += "</table>";
    document.getElementById("evaluationResults").innerHTML = resultsHtml;
}

/* -- Precision/Recall/F1 Calculation Utilities -- */
function precisionScore(dataArr, model, classIdx) {
    var tp = 0, fp = 0;
    for (var i = 0; i < dataArr.length; i++) {
        var pred = model.predict(dataArr[i].input);
        var predictedClass = getMaxIndex(pred);
        var actualClass = getMaxIndex(dataArr[i].target);
        if (predictedClass === classIdx) {
            if (actualClass === classIdx) tp++;
            else fp++;
        }
    }
    return (tp + fp) === 0 ? 0 : tp / (tp + fp);
}
function recallScore(dataArr, model, classIdx) {
    var tp = 0, fn = 0;
    for (var i = 0; i < dataArr.length; i++) {
        var pred = model.predict(dataArr[i].input);
        var predictedClass = getMaxIndex(pred);
        var actualClass = getMaxIndex(dataArr[i].target);
        if (actualClass === classIdx) {
            if (predictedClass === classIdx) tp++;
            else fn++;
        }
    }
    return (tp + fn) === 0 ? 0 : tp / (tp + fn);
}
function f1Score(prec, rec) {
    return (prec + rec) === 0 ? 0 : 2 * (prec * rec) / (prec + rec);
}

function getMaxIndex(arr) {
    var maxIdx = 0;
    for (var i = 1; i < arr.length; i++) {
        if (arr[i] > arr[maxIdx]) maxIdx = i;
    }
    return maxIdx;
}

/* ======= Save/load neural model as JSON ======= */
function saveModel() {
    if (!mlp) return;
    var s = mlp.toJSON();
    var blob = new Blob([s], {type: "application/json"});
    var a = document.createElement("a");
    a.href = URL.createObjectURL(blob);
    a.download = "mlp_model.json";
    a.click();
    document.getElementById("modelStatus").textContent = "Model saved!";
}
function loadModel() {
    var file = document.getElementById("modelFile").files[0];
    if (!file) return;
    var reader = new FileReader();
    reader.onload = function(e) {
        mlp = MLP.fromJSON(e.target.result);
        document.getElementById("modelStatus").textContent = "Model loaded!";
    };
    reader.readAsText(file);
}

/* ============= TRAIN NETWORK =============== */
function trainNetwork() {
    if (!mlp) return;
    var trainEpochs = parseInt(document.getElementById("trainingEpochs").value, 10);
    var batchSize = document.getElementById("batchSize").value;
    var totalSamples = data.length;
    var actualBatchSize = batchSize === 'all' ? totalSamples : parseInt(batchSize, 10);
    for (var ep = 0; ep < trainEpochs; ep++) {
        var shuffled = data.slice().sort(function(){return Math.random()-0.5;});
        for (var i = 0; i < totalSamples; i += actualBatchSize) {
            var batch = shuffled.slice(i, Math.min(i+actualBatchSize, totalSamples));
            for (var j = 0; j < batch.length; j++)
                mlp.train(batch[j].input, batch[j].target);
        }
    }
    document.getElementById("trainStatus").textContent = "Training complete (" + trainEpochs + " epochs)";
}
/* ========== Training with progress feedback (async UI) ========== */
function trainNetworkWithProgress() {
    if (!mlp) return;
    var trainEpochs = parseInt(document.getElementById("trainingEpochs").value, 10);
    var batchSize = document.getElementById("batchSize").value;
    var totalSamples = data.length;
    var actualBatchSize = batchSize === 'all' ? totalSamples : parseInt(batchSize, 10);
    var ep = 0;
    var trainStatus = document.getElementById("trainStatus");
    var trainProgress = document.getElementById("trainingProgress");
    trainStatus.textContent = '';
    function doEpoch() {
        var shuffled = data.slice().sort(function(){return Math.random()-0.5;});
        for (var i = 0; i < totalSamples; i += actualBatchSize) {
            var batch = shuffled.slice(i, Math.min(i+actualBatchSize, totalSamples));
            for (var j = 0; j < batch.length; j++)
                mlp.train(batch[j].input, batch[j].target);
        }
        ep++;
        trainProgress.textContent = "Epoch " + ep + "/" + trainEpochs;
        if (ep < trainEpochs) setTimeout(doEpoch, 30); // allow UI update
        else trainStatus.textContent = "Training complete (" + trainEpochs + " epochs)";
    }
    doEpoch();
}

/* ========== Predict on manual input ========== */
function predictInput() {
    if (!mlp) return;
    var vals = document.getElementById("predictInput").value.split(",").map(Number);
    var pred = mlp.predict(vals);
    document.getElementById("predictOutput").innerHTML = "Predicted: [" + pred.map(function(x){return x.toFixed(4);}).join(", ") + "]";
}
</script>
</body>
</html>
